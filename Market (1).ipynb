{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q1sDq83ryJXp"
      },
      "outputs": [],
      "source": [
        "# Step 1: å¯¼å…¥åº“\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1.1: å®šä¹‰ RSI å‡½æ•°\n",
        "def compute_rsi(series, period=14):\n",
        "    delta = series.diff()\n",
        "    gain = delta.clip(lower=0).rolling(window=period).mean()\n",
        "    loss = -delta.clip(upper=0).rolling(window=period).mean()\n",
        "    rs = gain / (loss + 1e-10)  # é˜²æ­¢é™¤ä»¥0\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n"
      ],
      "metadata": {
        "id": "E0T9Y6jUyplE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: è¯»å–æ•°æ®\n",
        "df = pd.read_csv('/content/Nasdaq100Data.csv', parse_dates=['Date'])\n",
        "\n",
        "# Step 2.1: è½¬æ¢æ•°å­—æ ¼å¼\n",
        "for col in ['Price', 'Open', 'High', 'Low']:\n",
        "    df[col] = df[col].astype(str).str.replace(',', '').astype(float)\n",
        "\n",
        "# Step 2.2: æ·»åŠ æŠ€æœ¯æŒ‡æ ‡\n",
        "df['MA10'] = df['Price'].rolling(10).mean()\n",
        "df['RSI'] = compute_rsi(df['Price'])\n",
        "# === æ·»åŠ å¢å¼ºç‰¹å¾ ===\n",
        "df['Return5'] = df['Price'].pct_change(5)\n",
        "df['Volatility5'] = df['Price'].rolling(5).std()\n",
        "df['Bias_MA10'] = (df['Price'] - df['MA10']) / df['MA10']\n",
        "\n",
        "# æœ€åæ›´æ–°ä½ çš„ç‰¹å¾åˆ—\n",
        "feature_cols = ['Price', 'MA10', 'RSI', 'Return5', 'Volatility5', 'Bias_MA10']\n",
        "\n",
        "\n",
        "# Step 2.3: åˆ é™¤NaN\n",
        "df.dropna(inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n"
      ],
      "metadata": {
        "id": "JcTvlJpvyKx9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ç¡®ä¿æŒ‰æ—¶é—´å‡åºæ’åˆ—å¹¶é‡ç½®ç´¢å¼•\n",
        "df = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "# æ ‡ç­¾æ„é€ ï¼šä¸¥æ ¼æ¯”è¾ƒä»Šå¤©ä¸æœªæ¥5å¤©æ¯ä¸€å¤©çš„ä»·æ ¼\n",
        "labels = []\n",
        "for i in range(len(df) - 5):\n",
        "    current = df.loc[i, 'Price']\n",
        "    future = df.loc[i+1:i+5, 'Price'].values\n",
        "\n",
        "    if np.all(current < future):\n",
        "        labels.append(1)  # å½“å‰æ¯”æœªæ¥éƒ½ä½ â†’ æœ€ä½ç‚¹ï¼ˆä¹°å…¥ï¼‰\n",
        "    elif np.all(current > future):\n",
        "        labels.append(2)  # å½“å‰æ¯”æœªæ¥éƒ½é«˜ â†’ æœ€é«˜ç‚¹ï¼ˆå–å‡ºï¼‰\n",
        "    else:\n",
        "        labels.append(0)  # ä¸­é—´åŒºåŸŸï¼Œä¸ç¡®å®š\n",
        "\n",
        "# å°¾éƒ¨è¡¥0ï¼ˆæœªæ¥ä¸è¶³5å¤©æ— æ³•åˆ¤æ–­ï¼‰\n",
        "labels += [0] * (len(df) - len(labels))\n",
        "\n",
        "df['label'] = labels\n"
      ],
      "metadata": {
        "id": "gD5Yma60yK2p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ\n",
        "unique, counts = np.unique(df['label'], return_counts=True)\n",
        "label_count = dict(zip(unique, counts))\n",
        "\n",
        "# æ˜¾ç¤ºç»“æœ\n",
        "print(\"\\n=== æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡ ===\")\n",
        "print(f\"ä¸ç¡®å®šï¼ˆlabel=0ï¼‰ï¼š{label_count.get(0, 0)} å¤©\")\n",
        "print(f\"æœ€ä½ç‚¹ï¼ˆlabel=1ï¼Œé«˜ç‚¹ä¿¡å·ï¼‰ï¼š{label_count.get(1, 0)} å¤©\")\n",
        "print(f\"æœ€é«˜ç‚¹ï¼ˆlabel=2ï¼Œä½ç‚¹ä¿¡å·ï¼‰ï¼š{label_count.get(2, 0)} å¤©\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDSmu1nW0dHX",
        "outputId": "1098ab82-f87d-4334-ccc6-45ba570cf002"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡ ===\n",
            "ä¸ç¡®å®šï¼ˆlabel=0ï¼‰ï¼š1175 å¤©\n",
            "æœ€ä½ç‚¹ï¼ˆlabel=1ï¼Œé«˜ç‚¹ä¿¡å·ï¼‰ï¼š699 å¤©\n",
            "æœ€é«˜ç‚¹ï¼ˆlabel=2ï¼Œä½ç‚¹ä¿¡å·ï¼‰ï¼š381 å¤©\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.loc[df['label'] == 1].tail(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9yXh4cm2N8_",
        "outputId": "e07ff1c5-f54e-4c44-bf77-dac4e4d01571"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Date     Price      Open      High       Low     Vol. Change %  \\\n",
            "2191 2025-01-27  21127.28  21000.17  21292.54  20974.90  631.40M   -2.97%   \n",
            "2196 2025-02-03  21297.58  21084.64  21406.58  21004.35  387.17M   -0.84%   \n",
            "2200 2025-02-07  21491.31  21776.28  21869.32  21465.46  323.70M   -1.30%   \n",
            "2202 2025-02-11  21693.52  21629.11  21776.25  21625.51  283.61M   -0.29%   \n",
            "2203 2025-02-12  21719.26  21475.41  21745.66  21454.19  308.36M    0.12%   \n",
            "2223 2025-03-13  19225.48  19534.37  19558.56  19152.57  398.60M   -1.89%   \n",
            "2226 2025-03-18  19483.36  19657.10  19676.05  19397.07  333.56M   -1.66%   \n",
            "2228 2025-03-20  19677.61  19558.28  19888.85  19549.31  371.49M   -0.30%   \n",
            "2241 2025-04-08  17090.40  18034.46  18207.01  16850.18  660.31M   -1.95%   \n",
            "2249 2025-04-21  17808.30  18023.01  18043.08  17592.92  347.85M   -2.46%   \n",
            "\n",
            "           MA10        RSI   Return5  Volatility5  Bias_MA10  label  \n",
            "2191  21477.598  27.595220 -0.007996   155.108243  -0.016311      1  \n",
            "2196  21710.294  48.861481 -0.021104   179.623824  -0.019010      1  \n",
            "2200  21882.857  60.609472 -0.028189   193.141854  -0.017893      1  \n",
            "2202  21801.985  71.887068 -0.021739   222.805528  -0.004975      1  \n",
            "2203  21745.925  66.459341 -0.015806   188.712292  -0.001226      1  \n",
            "2223  19777.922  44.346060 -0.022977   238.879664  -0.027932      1  \n",
            "2226  19739.534  73.051699 -0.039653   255.182131  -0.012978      1  \n",
            "2228  19719.352  54.074642 -0.006112   264.601710  -0.002117      1  \n",
            "2241  18349.577  29.922965 -0.092396   792.695425  -0.068622      1  \n",
            "2249  19185.751  11.023965 -0.083336   666.094441  -0.071796      1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jNPjABkB2V4J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# æ›¿æ¢ inf ä¸º NaNï¼ˆè¿™ä¸€æ­¥å¿…é¡»æ”¾å‰é¢ï¼‰\n",
        "df[feature_cols] = df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# è®°å½•è¢«åˆ é™¤çš„è¡Œ\n",
        "na_mask = df[feature_cols].isna()\n",
        "rows_with_na = df[na_mask.any(axis=1)].copy()\n",
        "\n",
        "print(f\"\\nğŸ§¹ åˆ é™¤å«æ— æ•ˆç‰¹å¾çš„è¡Œæ•°ï¼š{len(rows_with_na)} è¡Œ\")\n",
        "\n",
        "# è¾“å‡ºè¢«åˆ é™¤è¡Œçš„å…³é”®ä¿¡æ¯ï¼ˆæ—¥æœŸã€ä»·æ ¼ã€ç¼ºå¤±é¡¹ï¼‰\n",
        "for idx, row in rows_with_na.iterrows():\n",
        "    na_cols = na_mask.loc[idx]\n",
        "    missing_features = [col for col, is_na in na_cols.items() if is_na]\n",
        "    date = row['Date']\n",
        "    price = row['Price']\n",
        "    print(f\"ğŸ“‰ æ—¥æœŸ: {date.date()} | ä»·æ ¼: {price:.2f} | ç¼ºå¤±ç‰¹å¾: {', '.join(missing_features)}\")\n",
        "\n",
        "# æ‰§è¡Œåˆ é™¤å¹¶é‡ç½®ç´¢å¼•\n",
        "df = df.dropna(subset=feature_cols).reset_index(drop=True)\n",
        "\n",
        "# æ ‡å‡†åŒ–ç‰¹å¾\n",
        "scaler = MinMaxScaler()\n",
        "df[feature_cols] = scaler.fit_transform(df[feature_cols])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0JUHVc9zhWv",
        "outputId": "75c36eb7-abe2-4fe8-f9a7-2bcdf9fca7b0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ§¹ åˆ é™¤å«æ— æ•ˆç‰¹å¾çš„è¡Œæ•°ï¼š0 è¡Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y, indices = [], [], []\n",
        "\n",
        "for i in range(30, len(df)):\n",
        "    seq = df.loc[i-30:i-1, feature_cols].values\n",
        "    label = df.loc[i, 'label']\n",
        "\n",
        "    if not np.isfinite(seq).all():\n",
        "        continue\n",
        "    if seq.shape != (30, len(feature_cols)):\n",
        "        continue\n",
        "\n",
        "    X.append(seq)\n",
        "    y.append(label)\n",
        "    indices.append(i)  # è®°å½•å½“å‰æ ·æœ¬æ ‡ç­¾æ‰€åœ¨çš„æ—¥æœŸç´¢å¼•ï¼Œç”¨äºé¢„æµ‹å±•ç¤º\n",
        "\n",
        "X = np.array(X, dtype=np.float32)\n",
        "y = np.array(y, dtype=np.int32)\n"
      ],
      "metadata": {
        "id": "sPsLLHqJ4K3Y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nâœ… æœ€ç»ˆè®­ç»ƒæ ·æœ¬æ•°ï¼š{X.shape[0]}, è¾“å…¥ç»´åº¦ï¼š{X.shape[1:]}\")\n",
        "\n",
        "# æ ‡ç­¾ç»Ÿè®¡\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "print(\"\\nğŸ“Š æ ‡ç­¾åˆ†å¸ƒï¼š\")\n",
        "for u, c in zip(unique, counts):\n",
        "    name = {0: \"è§‚æœ›\", 1: \"ä¹°å…¥\", 2: \"å–å‡º\"}.get(u, \"æœªçŸ¥\")\n",
        "    print(f\"Label {u}ï¼ˆ{name}ï¼‰â†’ {c} æ¡\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_nxZqRU6A-2",
        "outputId": "838eb59d-8fbb-44de-a4a4-64284c9f9196"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… æœ€ç»ˆè®­ç»ƒæ ·æœ¬æ•°ï¼š2225, è¾“å…¥ç»´åº¦ï¼š(30, 6)\n",
            "\n",
            "ğŸ“Š æ ‡ç­¾åˆ†å¸ƒï¼š\n",
            "Label 0ï¼ˆè§‚æœ›ï¼‰â†’ 1158 æ¡\n",
            "Label 1ï¼ˆä¹°å…¥ï¼‰â†’ 694 æ¡\n",
            "Label 2ï¼ˆå–å‡ºï¼‰â†’ 373 æ¡\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# === è®¾ç½®ä½ è¦ç”¨çš„ç‰¹å¾åˆ—ï¼ˆè¿™äº›åˆ—å¿…é¡»å·²å­˜åœ¨ï¼‰ ===\n",
        "feature_cols = ['Price', 'MA10', 'RSI', 'Return5', 'Volatility5', 'Bias_MA10']\n",
        "\n",
        "# === æ ‡å‡†åŒ–ç‰¹å¾åˆ— ===\n",
        "scaler = MinMaxScaler()\n",
        "df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
        "\n",
        "# === æ„é€ æ ·æœ¬åºåˆ— X å’Œæ ‡ç­¾ y ===\n",
        "X, y, indices = [], [], []\n",
        "\n",
        "for i in range(30, len(df)):\n",
        "    seq = df.loc[i-30:i-1, feature_cols].values\n",
        "    label = df.loc[i, 'label']\n",
        "\n",
        "    if not np.isfinite(seq).all():\n",
        "        continue\n",
        "    if seq.shape != (30, len(feature_cols)):\n",
        "        continue\n",
        "\n",
        "    X.append(seq)\n",
        "    y.append(label)\n",
        "    indices.append(i)  # è®°å½•çœŸå®æ ‡ç­¾æ‰€åœ¨ä½ç½®ï¼ˆç¬¬iå¤©ï¼‰\n",
        "\n",
        "# è½¬æ¢ä¸º NumPy æ•°ç»„\n",
        "X = np.array(X, dtype=np.float32)\n",
        "y = np.array(y, dtype=np.int32)\n",
        "\n",
        "# === è¾“å‡ºç»´åº¦å’Œæ ‡ç­¾åˆ†å¸ƒ ===\n",
        "print(f\"\\nâœ… æœ€ç»ˆè®­ç»ƒæ ·æœ¬æ•°ï¼š{X.shape[0]}, æ¯ä¸ªæ ·æœ¬ç»´åº¦ï¼š{X.shape[1:]}\")\n",
        "\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "print(\"\\nğŸ“Š æ ‡ç­¾åˆ†å¸ƒï¼š\")\n",
        "for u, c in zip(unique, counts):\n",
        "    name = {0: \"è§‚æœ›\", 1: \"ä¹°å…¥\", 2: \"å–å‡º\"}.get(u, \"æœªçŸ¥\")\n",
        "    print(f\"Label {u}ï¼ˆ{name}ï¼‰â†’ {c} æ¡\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUUtXtpM-FZ0",
        "outputId": "0a1fca3e-9c0d-432d-acb3-0fce6943d7d4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… æœ€ç»ˆè®­ç»ƒæ ·æœ¬æ•°ï¼š2225, æ¯ä¸ªæ ·æœ¬ç»´åº¦ï¼š(30, 6)\n",
            "\n",
            "ğŸ“Š æ ‡ç­¾åˆ†å¸ƒï¼š\n",
            "Label 0ï¼ˆè§‚æœ›ï¼‰â†’ 1158 æ¡\n",
            "Label 1ï¼ˆä¹°å…¥ï¼‰â†’ 694 æ¡\n",
            "Label 2ï¼ˆå–å‡ºï¼‰â†’ 373 æ¡\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nğŸ“Œ æœ€å 10 æ¡è®­ç»ƒæ ·æœ¬ï¼ˆå«ç›®æ ‡æ—¥çš„æ—¥æœŸã€ä»·æ ¼ä¸æ ‡ç­¾ï¼‰ï¼š\")\n",
        "for i in range(-10, 0):\n",
        "    row_idx = indices[i]  # ç¬¬ i ä¸ªæ ·æœ¬çš„æ ‡ç­¾åœ¨åŸå§‹ df ä¸­çš„è¡Œå·\n",
        "    row = df.loc[row_idx]\n",
        "    label_text = {0: \"è§‚æœ›\", 1: \"ä¹°å…¥\", 2: \"å–å‡º\"}.get(row['label'], \"æœªçŸ¥\")\n",
        "    print(f\"{len(indices)+i+1:02d} | æ—¥æœŸ: {row['Date'].date()} | ä»·æ ¼: {row['Price']:.2f} | æ ‡ç­¾: {row['label']}ï¼ˆ{label_text}ï¼‰\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBOB-HhrIfDX",
        "outputId": "5b871cd9-9823-41bd-a76d-a112a3478621"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Œ æœ€å 10 æ¡è®­ç»ƒæ ·æœ¬ï¼ˆå«ç›®æ ‡æ—¥çš„æ—¥æœŸã€ä»·æ ¼ä¸æ ‡ç­¾ï¼‰ï¼š\n",
            "2216 | æ—¥æœŸ: 2025-04-14 | ä»·æ ¼: 0.81 | æ ‡ç­¾: 0ï¼ˆè§‚æœ›ï¼‰\n",
            "2217 | æ—¥æœŸ: 2025-04-15 | ä»·æ ¼: 0.81 | æ ‡ç­¾: 2ï¼ˆå–å‡ºï¼‰\n",
            "2218 | æ—¥æœŸ: 2025-04-16 | ä»·æ ¼: 0.78 | æ ‡ç­¾: 0ï¼ˆè§‚æœ›ï¼‰\n",
            "2219 | æ—¥æœŸ: 2025-04-17 | ä»·æ ¼: 0.78 | æ ‡ç­¾: 0ï¼ˆè§‚æœ›ï¼‰\n",
            "2220 | æ—¥æœŸ: 2025-04-21 | ä»·æ ¼: 0.76 | æ ‡ç­¾: 1ï¼ˆä¹°å…¥ï¼‰\n",
            "2221 | æ—¥æœŸ: 2025-04-22 | ä»·æ ¼: 0.78 | æ ‡ç­¾: 0ï¼ˆè§‚æœ›ï¼‰\n",
            "2222 | æ—¥æœŸ: 2025-04-23 | ä»·æ ¼: 0.81 | æ ‡ç­¾: 0ï¼ˆè§‚æœ›ï¼‰\n",
            "2223 | æ—¥æœŸ: 2025-04-24 | ä»·æ ¼: 0.84 | æ ‡ç­¾: 0ï¼ˆè§‚æœ›ï¼‰\n",
            "2224 | æ—¥æœŸ: 2025-04-25 | ä»·æ ¼: 0.85 | æ ‡ç­¾: 0ï¼ˆè§‚æœ›ï¼‰\n",
            "2225 | æ—¥æœŸ: 2025-04-28 | ä»·æ ¼: 0.85 | æ ‡ç­¾: 0ï¼ˆè§‚æœ›ï¼‰\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lV9E9aPD-gQF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4toUZjqv-ZGf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models, regularizers\n",
        "\n",
        "def build_cnn_model_3class(input_shape):\n",
        "    input_layer = layers.Input(shape=input_shape)\n",
        "\n",
        "    x = layers.Conv1D(64, 3, padding='same', activation='relu',\n",
        "                      kernel_regularizer=regularizers.l2(0.001))(input_layer)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv1D(64, 3, padding='same', activation='relu',\n",
        "                      kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    # ä¸‰åˆ†ç±»è¾“å‡ºï¼š0 = è§‚æœ›, 1 = ä¹°å…¥, 2 = å–å‡º\n",
        "    output = layers.Dense(3, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "KVPG4o-t8PJI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models, regularizers\n",
        "\n",
        "def build_stronger_cnn_model(input_shape):\n",
        "    input_layer = layers.Input(shape=input_shape)\n",
        "\n",
        "    # --- å¤šå°ºåº¦å·ç§¯ ---\n",
        "    conv3 = layers.Conv1D(64, 3, padding='same', activation='relu',\n",
        "                          kernel_regularizer=regularizers.l2(0.001))(input_layer)\n",
        "    conv5 = layers.Conv1D(64, 5, padding='same', activation='relu',\n",
        "                          kernel_regularizer=regularizers.l2(0.001))(input_layer)\n",
        "    conv7 = layers.Conv1D(64, 7, padding='same', activation='relu',\n",
        "                          kernel_regularizer=regularizers.l2(0.001))(input_layer)\n",
        "\n",
        "    x = layers.Concatenate()([conv3, conv5, conv7])\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # --- Global pooling ---\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "    # --- å…¨è¿æ¥ + Dropout ---\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    # --- ä¸‰åˆ†ç±»è¾“å‡º ---\n",
        "    output = layers.Dense(3, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "idcyiYf4DM86"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === ğŸ”¥ å¼ºåŒ–ç‰ˆ CNN æ¨¡å‹ï¼ˆä¸å« Attentionï¼‰===\n",
        "\n",
        "from tensorflow.keras import layers, models, regularizers, optimizers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "\n",
        "def build_stronger_cnn_model2(input_shape):\n",
        "    input_layer = layers.Input(shape=input_shape)\n",
        "\n",
        "    # --- å¤šå°ºåº¦å·ç§¯ ---\n",
        "    conv3 = layers.Conv1D(64, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001))(input_layer)\n",
        "    conv5 = layers.Conv1D(64, 5, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001))(input_layer)\n",
        "    conv7 = layers.Conv1D(64, 7, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001))(input_layer)\n",
        "\n",
        "    x = layers.Concatenate()([conv3, conv5, conv7])\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # --- å·ç§¯æ®‹å·®å—ï¼ˆå¢å¼ºéçº¿æ€§ + å±€éƒ¨æ„ŸçŸ¥ï¼‰ ---\n",
        "    res = layers.Conv1D(128, 3, padding='same', activation='relu')(x)\n",
        "    res = layers.BatchNormalization()(res)\n",
        "    res = layers.Conv1D(192, 3, padding='same', activation='relu')(res)\n",
        "    x = layers.Add()([x, res])\n",
        "    x = layers.LayerNormalization()(x)\n",
        "\n",
        "    # --- æ± åŒ– + å…¨è¿æ¥ ---\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    # --- è¾“å‡ºå±‚ï¼ˆä¸‰åˆ†ç±»ï¼‰---\n",
        "    output = layers.Dense(3, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "        loss=SparseCategoricalCrossentropy(),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "XgzW4jzbDt4M"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val, indices_train, indices_val = train_test_split(\n",
        "    X, y, indices, test_size=0.2, random_state=42, stratify=y\n",
        ")\n"
      ],
      "metadata": {
        "id": "un3vVq_0BqKH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = build_cnn_model_3class(input_shape=X.shape[1:])\n",
        "# model = build_stronger_cnn_model(input_shape=X.shape[1:])\n",
        "model = build_stronger_cnn_model2(input_shape=X.shape[1:])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "id": "xMqFTRaXBqNM",
        "outputId": "fc5561b1-8570-45b8-8d83-93ba6f25f679"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m6\u001b[0m)     â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
              "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)    â”‚      \u001b[38;5;34m1,216\u001b[0m â”‚ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)    â”‚      \u001b[38;5;34m1,984\u001b[0m â”‚ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)    â”‚      \u001b[38;5;34m2,752\u001b[0m â”‚ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ concatenate         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m192\u001b[0m)   â”‚          \u001b[38;5;34m0\u001b[0m â”‚ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     â”‚\n",
              "â”‚ (\u001b[38;5;33mConcatenate\u001b[0m)       â”‚                   â”‚            â”‚ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   â”‚\n",
              "â”‚                     â”‚                   â”‚            â”‚ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m192\u001b[0m)   â”‚        \u001b[38;5;34m768\u001b[0m â”‚ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
              "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)   â”‚     \u001b[38;5;34m73,856\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)   â”‚        \u001b[38;5;34m512\u001b[0m â”‚ conv1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
              "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m192\u001b[0m)   â”‚     \u001b[38;5;34m73,920\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ add (\u001b[38;5;33mAdd\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m192\u001b[0m)   â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
              "â”‚                     â”‚                   â”‚            â”‚ conv1d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ layer_normalization â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m192\u001b[0m)   â”‚        \u001b[38;5;34m384\u001b[0m â”‚ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         â”‚\n",
              "â”‚ (\u001b[38;5;33mLayerNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ global_max_poolingâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ layer_normalizatâ€¦ â”‚\n",
              "â”‚ (\u001b[38;5;33mGlobalMaxPooling1â€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚     \u001b[38;5;34m24,704\u001b[0m â”‚ global_max_pooliâ€¦ â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚      \u001b[38;5;34m8,256\u001b[0m â”‚ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         â”‚        \u001b[38;5;34m195\u001b[0m â”‚ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)     â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,216</span> â”‚ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,984</span> â”‚ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,752</span> â”‚ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ concatenate         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)   â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       â”‚                   â”‚            â”‚ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   â”‚\n",
              "â”‚                     â”‚                   â”‚            â”‚ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)   â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> â”‚ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> â”‚ batch_normalizatâ€¦ â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚ conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)   â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,920</span> â”‚ batch_normalizatâ€¦ â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)   â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
              "â”‚                     â”‚                   â”‚            â”‚ conv1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ layer_normalization â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)   â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> â”‚ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ global_max_poolingâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalizatâ€¦ â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1â€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> â”‚ global_max_pooliâ€¦ â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> â”‚ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m188,547\u001b[0m (736.51 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">188,547</span> (736.51 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m187,907\u001b[0m (734.01 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">187,907</span> (734.01 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m640\u001b[0m (2.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> (2.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss', patience=6, restore_best_weights=True, verbose=1\n",
        ")\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.5, patience=3, verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "3AtK0iUnBqPE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=80,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop, lr_scheduler],\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPBdLXhYBqRg",
        "outputId": "091aa07a-d194-474d-c2ca-c92127ea3c72"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 134ms/step - accuracy: 0.3645 - loss: 1.7599 - val_accuracy: 0.5371 - val_loss: 1.0561 - learning_rate: 1.0000e-04\n",
            "Epoch 2/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4574 - loss: 1.1947 - val_accuracy: 0.4899 - val_loss: 1.0684 - learning_rate: 1.0000e-04\n",
            "Epoch 3/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4813 - loss: 1.0911 - val_accuracy: 0.5146 - val_loss: 1.0563 - learning_rate: 1.0000e-04\n",
            "Epoch 4/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4682 - loss: 1.0583 - val_accuracy: 0.5551 - val_loss: 1.0215 - learning_rate: 1.0000e-04\n",
            "Epoch 5/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5433 - loss: 0.9892 - val_accuracy: 0.5865 - val_loss: 0.9880 - learning_rate: 1.0000e-04\n",
            "Epoch 6/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5499 - loss: 0.9550 - val_accuracy: 0.5955 - val_loss: 0.9543 - learning_rate: 1.0000e-04\n",
            "Epoch 7/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5518 - loss: 0.9451 - val_accuracy: 0.5798 - val_loss: 0.9361 - learning_rate: 1.0000e-04\n",
            "Epoch 8/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5428 - loss: 0.9231 - val_accuracy: 0.5910 - val_loss: 0.8972 - learning_rate: 1.0000e-04\n",
            "Epoch 9/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5778 - loss: 0.8871 - val_accuracy: 0.6090 - val_loss: 0.8705 - learning_rate: 1.0000e-04\n",
            "Epoch 10/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5905 - loss: 0.8671 - val_accuracy: 0.6360 - val_loss: 0.8366 - learning_rate: 1.0000e-04\n",
            "Epoch 11/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6032 - loss: 0.8671 - val_accuracy: 0.6494 - val_loss: 0.8201 - learning_rate: 1.0000e-04\n",
            "Epoch 12/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6232 - loss: 0.8303 - val_accuracy: 0.6225 - val_loss: 0.8247 - learning_rate: 1.0000e-04\n",
            "Epoch 13/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6465 - loss: 0.7836 - val_accuracy: 0.6112 - val_loss: 0.8343 - learning_rate: 1.0000e-04\n",
            "Epoch 14/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6563 - loss: 0.7677 - val_accuracy: 0.6562 - val_loss: 0.7794 - learning_rate: 1.0000e-04\n",
            "Epoch 15/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6364 - loss: 0.7832 - val_accuracy: 0.6652 - val_loss: 0.7580 - learning_rate: 1.0000e-04\n",
            "Epoch 16/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6523 - loss: 0.7557 - val_accuracy: 0.6652 - val_loss: 0.7545 - learning_rate: 1.0000e-04\n",
            "Epoch 17/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6636 - loss: 0.7596 - val_accuracy: 0.6629 - val_loss: 0.7687 - learning_rate: 1.0000e-04\n",
            "Epoch 18/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6742 - loss: 0.7223 - val_accuracy: 0.6742 - val_loss: 0.7464 - learning_rate: 1.0000e-04\n",
            "Epoch 19/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6775 - loss: 0.7345 - val_accuracy: 0.6674 - val_loss: 0.7490 - learning_rate: 1.0000e-04\n",
            "Epoch 20/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6571 - loss: 0.7309 - val_accuracy: 0.6764 - val_loss: 0.7401 - learning_rate: 1.0000e-04\n",
            "Epoch 21/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6575 - loss: 0.7164 - val_accuracy: 0.6742 - val_loss: 0.7254 - learning_rate: 1.0000e-04\n",
            "Epoch 22/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7174 - loss: 0.6753 - val_accuracy: 0.6787 - val_loss: 0.7433 - learning_rate: 1.0000e-04\n",
            "Epoch 23/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6955 - loss: 0.6927 - val_accuracy: 0.6899 - val_loss: 0.7309 - learning_rate: 1.0000e-04\n",
            "Epoch 24/80\n",
            "\u001b[1m55/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6737 - loss: 0.6867\n",
            "Epoch 24: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6743 - loss: 0.6862 - val_accuracy: 0.6697 - val_loss: 0.7297 - learning_rate: 1.0000e-04\n",
            "Epoch 25/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7253 - loss: 0.6329 - val_accuracy: 0.7079 - val_loss: 0.7017 - learning_rate: 5.0000e-05\n",
            "Epoch 26/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6997 - loss: 0.6473 - val_accuracy: 0.6944 - val_loss: 0.7116 - learning_rate: 5.0000e-05\n",
            "Epoch 27/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7330 - loss: 0.6276 - val_accuracy: 0.6966 - val_loss: 0.7140 - learning_rate: 5.0000e-05\n",
            "Epoch 28/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7218 - loss: 0.6249 - val_accuracy: 0.7034 - val_loss: 0.6947 - learning_rate: 5.0000e-05\n",
            "Epoch 29/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7401 - loss: 0.6016 - val_accuracy: 0.6899 - val_loss: 0.6993 - learning_rate: 5.0000e-05\n",
            "Epoch 30/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7356 - loss: 0.5889 - val_accuracy: 0.7169 - val_loss: 0.6819 - learning_rate: 5.0000e-05\n",
            "Epoch 31/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7700 - loss: 0.5589 - val_accuracy: 0.6966 - val_loss: 0.6973 - learning_rate: 5.0000e-05\n",
            "Epoch 32/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7434 - loss: 0.5888 - val_accuracy: 0.7079 - val_loss: 0.6793 - learning_rate: 5.0000e-05\n",
            "Epoch 33/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7642 - loss: 0.5642 - val_accuracy: 0.7146 - val_loss: 0.6694 - learning_rate: 5.0000e-05\n",
            "Epoch 34/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7631 - loss: 0.5529 - val_accuracy: 0.7079 - val_loss: 0.6711 - learning_rate: 5.0000e-05\n",
            "Epoch 35/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7601 - loss: 0.5526 - val_accuracy: 0.7011 - val_loss: 0.6748 - learning_rate: 5.0000e-05\n",
            "Epoch 36/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7751 - loss: 0.5740 - val_accuracy: 0.7034 - val_loss: 0.6667 - learning_rate: 5.0000e-05\n",
            "Epoch 37/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7730 - loss: 0.5344 - val_accuracy: 0.7281 - val_loss: 0.6549 - learning_rate: 5.0000e-05\n",
            "Epoch 38/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8192 - loss: 0.4765 - val_accuracy: 0.7236 - val_loss: 0.6699 - learning_rate: 5.0000e-05\n",
            "Epoch 39/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7670 - loss: 0.5158 - val_accuracy: 0.7371 - val_loss: 0.6618 - learning_rate: 5.0000e-05\n",
            "Epoch 40/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7756 - loss: 0.5075 - val_accuracy: 0.7393 - val_loss: 0.6441 - learning_rate: 5.0000e-05\n",
            "Epoch 41/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7922 - loss: 0.4900 - val_accuracy: 0.7393 - val_loss: 0.6501 - learning_rate: 5.0000e-05\n",
            "Epoch 42/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8196 - loss: 0.4622 - val_accuracy: 0.7483 - val_loss: 0.6147 - learning_rate: 5.0000e-05\n",
            "Epoch 43/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8329 - loss: 0.4567 - val_accuracy: 0.7506 - val_loss: 0.6208 - learning_rate: 5.0000e-05\n",
            "Epoch 44/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8054 - loss: 0.4637 - val_accuracy: 0.7528 - val_loss: 0.6145 - learning_rate: 5.0000e-05\n",
            "Epoch 45/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8131 - loss: 0.4638 - val_accuracy: 0.7551 - val_loss: 0.6156 - learning_rate: 5.0000e-05\n",
            "Epoch 46/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8357 - loss: 0.4308 - val_accuracy: 0.7573 - val_loss: 0.6136 - learning_rate: 5.0000e-05\n",
            "Epoch 47/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8389 - loss: 0.4041 - val_accuracy: 0.7596 - val_loss: 0.5968 - learning_rate: 5.0000e-05\n",
            "Epoch 48/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8554 - loss: 0.3998 - val_accuracy: 0.7618 - val_loss: 0.6103 - learning_rate: 5.0000e-05\n",
            "Epoch 49/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8325 - loss: 0.4111 - val_accuracy: 0.7708 - val_loss: 0.5932 - learning_rate: 5.0000e-05\n",
            "Epoch 50/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8381 - loss: 0.4054 - val_accuracy: 0.7663 - val_loss: 0.5941 - learning_rate: 5.0000e-05\n",
            "Epoch 51/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8445 - loss: 0.3912 - val_accuracy: 0.7573 - val_loss: 0.5710 - learning_rate: 5.0000e-05\n",
            "Epoch 52/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8492 - loss: 0.3818 - val_accuracy: 0.7708 - val_loss: 0.5693 - learning_rate: 5.0000e-05\n",
            "Epoch 53/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8491 - loss: 0.3767 - val_accuracy: 0.7910 - val_loss: 0.5744 - learning_rate: 5.0000e-05\n",
            "Epoch 54/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8429 - loss: 0.3970 - val_accuracy: 0.7798 - val_loss: 0.5833 - learning_rate: 5.0000e-05\n",
            "Epoch 55/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8742 - loss: 0.3610 - val_accuracy: 0.7888 - val_loss: 0.5655 - learning_rate: 5.0000e-05\n",
            "Epoch 56/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8642 - loss: 0.3564 - val_accuracy: 0.7843 - val_loss: 0.5916 - learning_rate: 5.0000e-05\n",
            "Epoch 57/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8848 - loss: 0.3423 - val_accuracy: 0.8000 - val_loss: 0.5561 - learning_rate: 5.0000e-05\n",
            "Epoch 58/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8657 - loss: 0.3573 - val_accuracy: 0.7865 - val_loss: 0.5510 - learning_rate: 5.0000e-05\n",
            "Epoch 59/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9021 - loss: 0.3114 - val_accuracy: 0.7888 - val_loss: 0.5548 - learning_rate: 5.0000e-05\n",
            "Epoch 60/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8806 - loss: 0.3357 - val_accuracy: 0.8045 - val_loss: 0.5468 - learning_rate: 5.0000e-05\n",
            "Epoch 61/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8862 - loss: 0.3129 - val_accuracy: 0.7933 - val_loss: 0.5464 - learning_rate: 5.0000e-05\n",
            "Epoch 62/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8971 - loss: 0.3036 - val_accuracy: 0.7843 - val_loss: 0.5447 - learning_rate: 5.0000e-05\n",
            "Epoch 63/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8727 - loss: 0.3389 - val_accuracy: 0.7910 - val_loss: 0.5614 - learning_rate: 5.0000e-05\n",
            "Epoch 64/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8914 - loss: 0.2953 - val_accuracy: 0.7888 - val_loss: 0.5308 - learning_rate: 5.0000e-05\n",
            "Epoch 65/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8997 - loss: 0.2890 - val_accuracy: 0.7978 - val_loss: 0.5350 - learning_rate: 5.0000e-05\n",
            "Epoch 66/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9036 - loss: 0.2795 - val_accuracy: 0.7910 - val_loss: 0.5714 - learning_rate: 5.0000e-05\n",
            "Epoch 67/80\n",
            "\u001b[1m39/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9208 - loss: 0.2522\n",
            "Epoch 67: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9164 - loss: 0.2602 - val_accuracy: 0.7955 - val_loss: 0.5496 - learning_rate: 5.0000e-05\n",
            "Epoch 68/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8980 - loss: 0.2867 - val_accuracy: 0.8000 - val_loss: 0.5400 - learning_rate: 2.5000e-05\n",
            "Epoch 69/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9357 - loss: 0.2386 - val_accuracy: 0.7888 - val_loss: 0.5222 - learning_rate: 2.5000e-05\n",
            "Epoch 70/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9120 - loss: 0.2749 - val_accuracy: 0.8045 - val_loss: 0.5263 - learning_rate: 2.5000e-05\n",
            "Epoch 71/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9198 - loss: 0.2637 - val_accuracy: 0.8022 - val_loss: 0.5560 - learning_rate: 2.5000e-05\n",
            "Epoch 72/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9075 - loss: 0.2530\n",
            "Epoch 72: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9076 - loss: 0.2530 - val_accuracy: 0.7933 - val_loss: 0.5349 - learning_rate: 2.5000e-05\n",
            "Epoch 73/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9176 - loss: 0.2421 - val_accuracy: 0.8022 - val_loss: 0.5469 - learning_rate: 1.2500e-05\n",
            "Epoch 74/80\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9294 - loss: 0.2343 - val_accuracy: 0.7888 - val_loss: 0.5252 - learning_rate: 1.2500e-05\n",
            "Epoch 75/80\n",
            "\u001b[1m40/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9255 - loss: 0.2514\n",
            "Epoch 75: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
            "\u001b[1m56/56\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9250 - loss: 0.2512 - val_accuracy: 0.8067 - val_loss: 0.5412 - learning_rate: 1.2500e-05\n",
            "Epoch 75: early stopping\n",
            "Restoring model weights from the end of the best epoch: 69.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# === æ¨ç†é¢„æµ‹ ===\n",
        "pred_probs = model.predict(X_val)\n",
        "pred_labels = np.argmax(pred_probs, axis=1)\n",
        "\n",
        "# === æ ‡ç­¾æ˜ å°„ï¼ˆå¯è‡ªå®šä¹‰ï¼‰ ===\n",
        "label_names = {0: \"è§‚æœ›\", 1: \"ä¹°å…¥\", 2: \"å–å‡º\"}\n",
        "\n",
        "# === æ‰“å°å‰ N æ¡é¢„æµ‹ç»“æœï¼ˆå«æ—¶é—´ä¸ä»·æ ¼ï¼‰ ===\n",
        "print(\"\\nğŸ“Š æµ‹è¯•é›†é¢„æµ‹ç»“æœï¼ˆå‰20æ¡ï¼‰ï¼š\")\n",
        "for i in range(min(20, len(X_val))):\n",
        "    idx = indices_val[i]  # æ‰¾å›å¯¹åº”çš„ df è¡Œ\n",
        "    date = df.loc[idx, 'Date']\n",
        "    price = df.loc[idx, 'Price']\n",
        "    actual = y_val[i]\n",
        "    pred = pred_labels[i]\n",
        "    prob = pred_probs[i]\n",
        "\n",
        "    print(f\"{i+1:02d} | æ—¥æœŸ: {date.date()} | ä»·æ ¼: {price:.2f} | \"\n",
        "          f\"é¢„æµ‹: {label_names[pred]} ({prob[pred]:.2f}) | å®é™…: {label_names[actual]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcpwfVRLBqT0",
        "outputId": "2de5200e-a7c4-4792-8af8-4b5115bf293f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
            "\n",
            "ğŸ“Š æµ‹è¯•é›†é¢„æµ‹ç»“æœï¼ˆå‰20æ¡ï¼‰ï¼š\n",
            "01 | æ—¥æœŸ: 2023-09-07 | ä»·æ ¼: 0.62 | é¢„æµ‹: è§‚æœ› (0.51) | å®é™…: è§‚æœ›\n",
            "02 | æ—¥æœŸ: 2017-03-10 | ä»·æ ¼: 0.07 | é¢„æµ‹: è§‚æœ› (0.72) | å®é™…: è§‚æœ›\n",
            "03 | æ—¥æœŸ: 2019-10-07 | ä»·æ ¼: 0.20 | é¢„æµ‹: è§‚æœ› (0.94) | å®é™…: è§‚æœ›\n",
            "04 | æ—¥æœŸ: 2023-09-14 | ä»·æ ¼: 0.63 | é¢„æµ‹: è§‚æœ› (0.76) | å®é™…: è§‚æœ›\n",
            "05 | æ—¥æœŸ: 2020-02-19 | ä»·æ ¼: 0.31 | é¢„æµ‹: è§‚æœ› (0.93) | å®é™…: è§‚æœ›\n",
            "06 | æ—¥æœŸ: 2020-08-26 | ä»·æ ¼: 0.43 | é¢„æµ‹: è§‚æœ› (0.99) | å®é™…: è§‚æœ›\n",
            "07 | æ—¥æœŸ: 2018-05-03 | ä»·æ ¼: 0.14 | é¢„æµ‹: è§‚æœ› (0.99) | å®é™…: è§‚æœ›\n",
            "08 | æ—¥æœŸ: 2024-12-12 | ä»·æ ¼: 0.97 | é¢„æµ‹: ä¹°å…¥ (0.82) | å®é™…: ä¹°å…¥\n",
            "09 | æ—¥æœŸ: 2018-09-21 | ä»·æ ¼: 0.19 | é¢„æµ‹: è§‚æœ› (0.76) | å®é™…: ä¹°å…¥\n",
            "10 | æ—¥æœŸ: 2021-11-10 | ä»·æ ¼: 0.66 | é¢„æµ‹: ä¹°å…¥ (0.85) | å®é™…: ä¹°å…¥\n",
            "11 | æ—¥æœŸ: 2023-07-11 | ä»·æ ¼: 0.61 | é¢„æµ‹: è§‚æœ› (0.95) | å®é™…: è§‚æœ›\n",
            "12 | æ—¥æœŸ: 2024-01-11 | ä»·æ ¼: 0.70 | é¢„æµ‹: ä¹°å…¥ (0.88) | å®é™…: ä¹°å…¥\n",
            "13 | æ—¥æœŸ: 2018-09-20 | ä»·æ ¼: 0.19 | é¢„æµ‹: è§‚æœ› (0.99) | å®é™…: è§‚æœ›\n",
            "14 | æ—¥æœŸ: 2019-06-07 | ä»·æ ¼: 0.18 | é¢„æµ‹: ä¹°å…¥ (0.73) | å®é™…: ä¹°å…¥\n",
            "15 | æ—¥æœŸ: 2018-04-13 | ä»·æ ¼: 0.14 | é¢„æµ‹: è§‚æœ› (0.99) | å®é™…: è§‚æœ›\n",
            "16 | æ—¥æœŸ: 2017-01-17 | ä»·æ ¼: 0.05 | é¢„æµ‹: ä¹°å…¥ (0.50) | å®é™…: è§‚æœ›\n",
            "17 | æ—¥æœŸ: 2018-05-30 | ä»·æ ¼: 0.15 | é¢„æµ‹: å–å‡º (0.88) | å®é™…: å–å‡º\n",
            "18 | æ—¥æœŸ: 2024-04-29 | ä»·æ ¼: 0.76 | é¢„æµ‹: ä¹°å…¥ (0.64) | å®é™…: ä¹°å…¥\n",
            "19 | æ—¥æœŸ: 2021-02-23 | ä»·æ ¼: 0.50 | é¢„æµ‹: å–å‡º (0.60) | å®é™…: å–å‡º\n",
            "20 | æ—¥æœŸ: 2022-01-20 | ä»·æ ¼: 0.59 | é¢„æµ‹: è§‚æœ› (0.96) | å®é™…: è§‚æœ›\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# æ¨¡å‹é¢„æµ‹\n",
        "pred_probs = model.predict(X_val)\n",
        "pred_labels = np.argmax(pred_probs, axis=1)\n",
        "\n",
        "# å®é™…æ ‡ç­¾\n",
        "actual = y_val\n",
        "\n",
        "# é¢„æµ‹ä¸ºä¹°å…¥ï¼ˆ1ï¼‰\n",
        "pred_buy_mask = pred_labels == 1\n",
        "buy_correct = np.sum((pred_labels == 1) & (actual == 1))\n",
        "buy_total = np.sum(pred_buy_mask)\n",
        "buy_precision = buy_correct / buy_total if buy_total > 0 else 0\n",
        "\n",
        "# é¢„æµ‹ä¸ºå–å‡ºï¼ˆ2ï¼‰\n",
        "pred_sell_mask = pred_labels == 2\n",
        "sell_correct = np.sum((pred_labels == 2) & (actual == 2))\n",
        "sell_total = np.sum(pred_sell_mask)\n",
        "sell_precision = sell_correct / sell_total if sell_total > 0 else 0\n",
        "\n",
        "# è¾“å‡º\n",
        "print(f\"\\nğŸ¯ æ¨¡å‹é¢„æµ‹ç²¾åº¦åˆ†æï¼š\")\n",
        "print(f\"ğŸŸ¢ é¢„æµ‹ä¸ºã€ä¹°å…¥ã€æ—¶çš„å‡†ç¡®ç‡ï¼š{buy_precision:.2%}ï¼ˆ{buy_correct}/{buy_total}ï¼‰\")\n",
        "print(f\"ğŸ”´ é¢„æµ‹ä¸ºã€å–å‡ºã€æ—¶çš„å‡†ç¡®ç‡ï¼š{sell_precision:.2%}ï¼ˆ{sell_correct}/{sell_total}ï¼‰\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEDaf32oCRGq",
        "outputId": "a0423630-4619-4c73-b275-aacfaa5e5e37"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\n",
            "ğŸ¯ æ¨¡å‹é¢„æµ‹ç²¾åº¦åˆ†æï¼š\n",
            "ğŸŸ¢ é¢„æµ‹ä¸ºã€ä¹°å…¥ã€æ—¶çš„å‡†ç¡®ç‡ï¼š78.32%ï¼ˆ112/143ï¼‰\n",
            "ğŸ”´ é¢„æµ‹ä¸ºã€å–å‡ºã€æ—¶çš„å‡†ç¡®ç‡ï¼š77.14%ï¼ˆ54/70ï¼‰\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"models/stock_cnn_model.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbc_eYTGFwe_",
        "outputId": "fde2d3cb-eb01-42a1-be83-523c639c05f7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_latest_from_raw_csv(csv_path, model_path, feature_cols, window=30):\n",
        "    # === åŠ è½½æ¨¡å‹ ===\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    # === åŸå§‹æ•°æ®å¤„ç† ===\n",
        "    df = pd.read_csv(csv_path, parse_dates=['Date'])\n",
        "\n",
        "    for col in ['Price', 'Open', 'High', 'Low']:\n",
        "        df[col] = df[col].astype(str).str.replace(',', '').astype(float)\n",
        "\n",
        "    # å‡åºæ’åˆ—æ—¥æœŸï¼Œç¡®ä¿ tail() æ‹¿åˆ°çš„æ˜¯æœ€æ–°çš„30å¤©\n",
        "    df = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "    # æ·»åŠ æŠ€æœ¯æŒ‡æ ‡ï¼ˆå’Œè®­ç»ƒä¿æŒä¸€è‡´ï¼‰\n",
        "    df['MA10'] = df['Price'].rolling(10).mean()\n",
        "    df['RSI'] = compute_rsi(df['Price'])\n",
        "    df['Return5'] = df['Price'].pct_change(5)\n",
        "    df['Volatility5'] = df['Price'].rolling(5).std()\n",
        "    df['Bias_MA10'] = (df['Price'] - df['MA10']) / df['MA10']\n",
        "\n",
        "    df[feature_cols] = df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # å–æœ€æ–° window å¤©\n",
        "    df_latest = df.tail(window).copy()\n",
        "\n",
        "    if df_latest.shape[0] < window or df_latest[feature_cols].isna().any().any():\n",
        "        print(\"âš ï¸ æœ€æ–°30å¤©æ ·æœ¬ä¸å®Œæ•´ï¼Œæ— æ³•æ¨ç†\")\n",
        "        return\n",
        "\n",
        "    # æ ‡å‡†åŒ–ç‰¹å¾\n",
        "    scaler = MinMaxScaler()\n",
        "    df_latest[feature_cols] = scaler.fit_transform(df_latest[feature_cols])\n",
        "\n",
        "    X_latest = np.expand_dims(df_latest[feature_cols].values, axis=0).astype(np.float32)\n",
        "\n",
        "    # === æ¨ç† ===\n",
        "    prob = model.predict(X_latest)[0]\n",
        "    label = np.argmax(prob)\n",
        "    label_map = {0: \"è§‚æœ›\", 1: \"ä¹°å…¥\", 2: \"å–å‡º\"}\n",
        "\n",
        "    latest_date = df_latest['Date'].iloc[-1]\n",
        "    latest_price = df_latest['Price'].iloc[-1]\n",
        "\n",
        "    print(f\"\\nğŸ“… æœ€æ–°æ—¥æœŸï¼š{latest_date.date()} | å½“å‰ä»·æ ¼ï¼š{latest_price:.2f}\")\n",
        "    print(f\"ğŸ¤– æ¨¡å‹é¢„æµ‹ï¼š{label_map[label]}ï¼ˆæ¦‚ç‡ï¼š{prob[label]:.2%}ï¼‰\")\n"
      ],
      "metadata": {
        "id": "F35RJEGuFwkm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols = ['Price', 'MA10', 'RSI', 'Return5', 'Volatility5', 'Bias_MA10']\n",
        "\n",
        "predict_latest_from_raw_csv(\n",
        "    csv_path='Nasdaq100Data.csv',\n",
        "    model_path='models/stock_cnn_model.h5',\n",
        "    feature_cols=feature_cols\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTG3UzwDJBuv",
        "outputId": "713e47c8-1b5a-4eaf-a988-3fd173a92119"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 574ms/step\n",
            "\n",
            "ğŸ“… æœ€æ–°æ—¥æœŸï¼š2025-05-16 | å½“å‰ä»·æ ¼ï¼š1.00\n",
            "ğŸ¤– æ¨¡å‹é¢„æµ‹ï¼šè§‚æœ›ï¼ˆæ¦‚ç‡ï¼š99.45%ï¼‰\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_recent_n_days(csv_path, model_path, feature_cols, window=30, n_days=20):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from tensorflow.keras.models import load_model\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "    def compute_rsi(series, period=14):\n",
        "        delta = series.diff()\n",
        "        gain = delta.clip(lower=0).rolling(window=period).mean()\n",
        "        loss = -delta.clip(upper=0).rolling(window=period).mean()\n",
        "        rs = gain / (loss + 1e-10)\n",
        "        return 100 - (100 / (1 + rs))\n",
        "\n",
        "    # === åŠ è½½æ¨¡å‹ ===\n",
        "    model = load_model(model_path)\n",
        "    df = pd.read_csv(csv_path, parse_dates=['Date'])\n",
        "\n",
        "    for col in ['Price', 'Open', 'High', 'Low']:\n",
        "        df[col] = df[col].astype(str).str.replace(',', '').astype(float)\n",
        "\n",
        "    df = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "    # æ·»åŠ æŒ‡æ ‡\n",
        "    df['MA10'] = df['Price'].rolling(10).mean()\n",
        "    df['RSI'] = compute_rsi(df['Price'])\n",
        "    df['Return5'] = df['Price'].pct_change(5)\n",
        "    df['Volatility5'] = df['Price'].rolling(5).std()\n",
        "    df['Bias_MA10'] = (df['Price'] - df['MA10']) / df['MA10']\n",
        "\n",
        "    df[feature_cols] = df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
        "    df = df.dropna(subset=feature_cols).reset_index(drop=True)\n",
        "\n",
        "    if len(df) < window + n_days:\n",
        "        print(\"âŒ æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆæœ€è¿‘ N å¤©çš„æ¨ç†æ ·æœ¬\")\n",
        "        return\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
        "\n",
        "    print(f\"\\nğŸ“Š æœ€è¿‘ {n_days} å¤©é¢„æµ‹ï¼ˆé«˜ç½®ä¿¡åº¦æç¤º ğŸŒŸï¼‰ï¼š\")\n",
        "    label_map = {0: \"è§‚æœ›\", 1: \"ä¹°å…¥\", 2: \"å–å‡º\"}\n",
        "\n",
        "    for i in range(-n_days, 0):\n",
        "        if i - window < -len(df):\n",
        "            continue\n",
        "        X_seq = df[feature_cols].iloc[i - window:i].values\n",
        "        if X_seq.shape != (window, len(feature_cols)):\n",
        "            continue\n",
        "\n",
        "        X_input = np.expand_dims(X_seq, axis=0).astype(np.float32)\n",
        "        prob = model.predict(X_input, verbose=0)[0]\n",
        "        label = np.argmax(prob)\n",
        "        confidence = prob[label]\n",
        "\n",
        "        date = df['Date'].iloc[i]\n",
        "        price = df['Price'].iloc[i]\n",
        "\n",
        "        # é«˜ç½®ä¿¡åº¦æ ‡è®°\n",
        "        marker = \" ğŸŒŸ é«˜ç½®ä¿¡åº¦ï¼\" if confidence > 0.85 else \"\"\n",
        "\n",
        "        print(f\"{date.date()} | ğŸ’°{price:.2f} | é¢„æµ‹ï¼š{label_map[label]}ï¼ˆæ¦‚ç‡ï¼š{confidence:.2%}ï¼‰{marker}\")\n"
      ],
      "metadata": {
        "id": "fSFmLZsVJt4n"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols = ['Price', 'MA10', 'RSI', 'Return5', 'Volatility5', 'Bias_MA10']\n",
        "predict_recent_n_days(\n",
        "    csv_path='Nasdaq100Data.csv',\n",
        "    model_path='models/stock_cnn_model.h5',\n",
        "    feature_cols=feature_cols,\n",
        "    window=30,\n",
        "    n_days=20\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS3XuIT3Juon",
        "outputId": "c32958bb-fb2f-4315-c69f-c5266050462e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š æœ€è¿‘ 20 å¤©é¢„æµ‹ï¼ˆé«˜ç½®ä¿¡åº¦æç¤º ğŸŒŸï¼‰ï¼š\n",
            "2025-04-21 | ğŸ’°0.76 | é¢„æµ‹ï¼šè§‚æœ›ï¼ˆæ¦‚ç‡ï¼š94.85%ï¼‰ ğŸŒŸ é«˜ç½®ä¿¡åº¦ï¼\n",
            "2025-04-22 | ğŸ’°0.78 | é¢„æµ‹ï¼šä¹°å…¥ï¼ˆæ¦‚ç‡ï¼š74.89%ï¼‰\n",
            "2025-04-23 | ğŸ’°0.81 | é¢„æµ‹ï¼šä¹°å…¥ï¼ˆæ¦‚ç‡ï¼š95.37%ï¼‰ ğŸŒŸ é«˜ç½®ä¿¡åº¦ï¼\n",
            "2025-04-24 | ğŸ’°0.84 | é¢„æµ‹ï¼šè§‚æœ›ï¼ˆæ¦‚ç‡ï¼š95.28%ï¼‰ ğŸŒŸ é«˜ç½®ä¿¡åº¦ï¼\n",
            "2025-04-25 | ğŸ’°0.85 | é¢„æµ‹ï¼šè§‚æœ›ï¼ˆæ¦‚ç‡ï¼š77.01%ï¼‰\n",
            "2025-04-28 | ğŸ’°0.85 | é¢„æµ‹ï¼šè§‚æœ›ï¼ˆæ¦‚ç‡ï¼š88.15%ï¼‰ ğŸŒŸ é«˜ç½®ä¿¡åº¦ï¼\n",
            "2025-04-29 | ğŸ’°0.85 | é¢„æµ‹ï¼šè§‚æœ›ï¼ˆæ¦‚ç‡ï¼š88.66%ï¼‰ ğŸŒŸ é«˜ç½®ä¿¡åº¦ï¼\n",
            "2025-04-30 | ğŸ’°0.86 | é¢„æµ‹ï¼šè§‚æœ›ï¼ˆæ¦‚ç‡ï¼š95.72%ï¼‰ ğŸŒŸ é«˜ç½®ä¿¡åº¦ï¼\n",
            "2025-05-01 | ğŸ’°0.87 | é¢„æµ‹ï¼šè§‚æœ›ï¼ˆæ¦‚ç‡ï¼š98.27%ï¼‰ ğŸŒŸ é«˜ç½®ä¿¡åº¦ï¼\n",
            "2025-05-02 | ğŸ’°0.88 | é¢„æµ‹ï¼šè§‚æœ›ï¼ˆæ¦‚ç‡ï¼š97.21%ï¼‰ ğŸŒŸ é«˜ç½®ä¿¡åº¦ï¼\n",
            "2025-05-05 | ğŸ’°0.88 | é¢„æµ‹ï¼šè§‚æœ›ï¼ˆæ¦‚ç‡ï¼š96.31%ï¼‰ ğŸŒŸ é«˜ç½®ä¿¡åº¦ï¼\n",
            "2025-05-06 | ğŸ’°0.87 | é¢„æµ‹ï¼šè§‚æœ›ï¼ˆæ¦‚ç‡ï¼š95.86%ï¼‰ ğŸŒŸ é«˜ç½®ä¿¡åº¦ï¼\n",
            "2025-05-07 | ğŸ’°0.87 | é¢„æµ‹ï¼šè§‚æœ›ï¼ˆæ¦‚ç‡ï¼š96.42%ï¼‰ ğŸŒŸ é«˜ç½®ä¿¡åº¦ï¼\n",
            "2025-05-08 | ğŸ’°0.88 | é¢„æµ‹ï¼šè§‚æœ›ï¼ˆæ¦‚ç‡ï¼š98.15%ï¼‰ ğŸŒŸ é«˜ç½®ä¿¡åº¦ï¼\n",
            "2025-05-09 | ğŸ’°0.88 | é¢„æµ‹ï¼šè§‚æœ›ï¼ˆæ¦‚ç‡ï¼š98.59%ï¼‰ ğŸŒŸ é«˜ç½®ä¿¡åº¦ï¼\n",
            "2025-05-12 | ğŸ’°0.93 | é¢„æµ‹ï¼šè§‚æœ›ï¼ˆæ¦‚ç‡ï¼š97.30%ï¼‰ ğŸŒŸ é«˜ç½®ä¿¡åº¦ï¼\n",
            "2025-05-13 | ğŸ’°0.95 | é¢„æµ‹ï¼šè§‚æœ›ï¼ˆæ¦‚ç‡ï¼š95.65%ï¼‰ ğŸŒŸ é«˜ç½®ä¿¡åº¦ï¼\n",
            "2025-05-14 | ğŸ’°0.95 | é¢„æµ‹ï¼šè§‚æœ›ï¼ˆæ¦‚ç‡ï¼š94.03%ï¼‰ ğŸŒŸ é«˜ç½®ä¿¡åº¦ï¼\n",
            "2025-05-15 | ğŸ’°0.95 | é¢„æµ‹ï¼šè§‚æœ›ï¼ˆæ¦‚ç‡ï¼š95.17%ï¼‰ ğŸŒŸ é«˜ç½®ä¿¡åº¦ï¼\n",
            "2025-05-16 | ğŸ’°0.96 | é¢„æµ‹ï¼šè§‚æœ›ï¼ˆæ¦‚ç‡ï¼š95.91%ï¼‰ ğŸŒŸ é«˜ç½®ä¿¡åº¦ï¼\n"
          ]
        }
      ]
    }
  ]
}